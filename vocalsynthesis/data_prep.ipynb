{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# python vocalsynthesis \n",
    "# python vocalsynthesis 20\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wave\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as tensor\n",
    "\n",
    "from blocks.model import Model\n",
    "from blocks.bricks import Linear, Tanh, Logistic #@list: changed from Sigmoid\n",
    "from blocks.bricks.cost import SquaredError\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "from fuel.datasets import IterableDataset\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import Transformer\n",
    "from blocks.algorithms import (GradientDescent, Scale, StepClipping, CompositeRule)\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.extensions import FinishAfter, Printing\n",
    "from blocks.bricks.recurrent import LSTM\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "\n",
    "# =========================================\n",
    "# SETTING UP DATA\n",
    "# =========================================\n",
    "\n",
    "# ==============\n",
    "# things that should be argparsed maybe\n",
    "# ==============\n",
    "datafile = 'song.wav'\n",
    "train_test_val_split = [80,10,10]\n",
    "data_start = 2000\n",
    "data_end = -150000\n",
    "window_shift = 1000\n",
    "x_length = 8000\n",
    "seq_length = 25\n",
    "batch_size = 100\n",
    "truncated_BPTT_length = 100\n",
    "secs_to_generate = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "song.wav\n",
      "16000 fps, 174077537 frames\n",
      "10879 seconds (181 minutes)\n",
      "\n",
      "\n",
      "Preprocessing...\n",
      "\n",
      "cut up in 80:10:10 train:test:val split\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============\n",
    "# load data and cut it up\n",
    "# =============\n",
    "print '\\nLoading data...\\n'\n",
    "\n",
    "data = wave.read(datafile)\n",
    "sec = data[0]\n",
    "minute = sec*60\n",
    "raw = data[1]\n",
    "track = raw[data_start:data_end]\n",
    "num_frames = len(track)\n",
    "length_in_seconds = num_frames/sec\n",
    "length_in_minutes = length_in_seconds/60\n",
    "length_in_hours = length_in_minutes/60\n",
    "\n",
    "print datafile\n",
    "print str(sec) + ' fps, '+ str(num_frames) + ' frames'\n",
    "print str(length_in_seconds) +' seconds (' + str(length_in_minutes) + ' minutes)'\n",
    "\n",
    "print '\\n\\nPreprocessing...\\n'\n",
    "\n",
    "test_start = int(num_frames*(train_test_val_split[0]/100.0))\n",
    "val_start = int(num_frames*(train_test_val_split[1]/100.0)+ test_start)\n",
    "\n",
    "train_data = np.asarray(raw[0:test_start], dtype=\"float32\")\n",
    "test_data = np.asarray(raw[test_start:val_start], dtype=\"float32\")\n",
    "val_data = np.asarray(raw[val_start: ], dtype=\"float32\")\n",
    "\n",
    "print 'cut up in ' + ':'.join([str(x) for x in train_test_val_split]) +' train:test:val split'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test min greater than train min; using train min to normalize\n",
      "test max less than train max; using train max to normalize\n",
      "val min greater than train min; using train min to normalize\n",
      "val max less than train max; using train max to normalize\n",
      "subtracted mean and normalized data to [-1,1]\n"
     ]
    }
   ],
   "source": [
    "# =============\n",
    "# subtract mean and normalize \n",
    "# =============\n",
    "train_mean = np.mean(train_data)\n",
    "train_min = np.min(train_data)\n",
    "train_max = np.max(train_data)\n",
    "train_range = train_max-train_min\n",
    "\n",
    "test_mean = np.mean(test_data)\n",
    "test_min = np.min(test_data)\n",
    "test_max = np.max(test_data)\n",
    "if test_min > train_min:\n",
    "    test_min = train_min\n",
    "    print \"test min greater than train min; using train min to normalize\"\n",
    "if test_max < train_max:\n",
    "    test_max = train_max\n",
    "    print \"test max less than train max; using train max to normalize\"\n",
    "test_range = test_max-test_min\n",
    "\n",
    "val_mean = np.mean(val_data)\n",
    "val_min = np.min(val_data)\n",
    "val_max = np.max(val_data)\n",
    "val_range = val_max-val_min\n",
    "if val_min > train_min:\n",
    "    val_min = train_min\n",
    "    print \"val min greater than train min; using train min to normalize\"\n",
    "if val_max < train_max:\n",
    "    val_max = train_max\n",
    "    print \"val max less than train max; using train max to normalize\"\n",
    "val_range = val_max-val_min\n",
    "\n",
    "#print '\\nTrain'\n",
    "#print train_mean \n",
    "#print train_min  \n",
    "#print train_max\n",
    "#print train_range\n",
    "\n",
    "#print '\\nTest'\n",
    "#print test_mean\n",
    "#print test_min \n",
    "#print test_max \n",
    "#print test_range\n",
    "\n",
    "#print '\\nVal'\n",
    "#print val_mean\n",
    "#print val_min \n",
    "#print val_max\n",
    "#print val_range\n",
    "\n",
    "#http://stackoverflow.com/questions/5294955/how-to-scale-down-a-range-of-numbers-with-a-known-min-and-max-value\n",
    "def rescale(unscaled_x, min_allowed, max_allowed, data_min, data_max):\n",
    "    return (max_allowed-min_allowed)*(unscaled_x-data_min)/(data_max-data_min) + min_allowed\n",
    "\n",
    "processed_train_data = rescale(train_data, -1,1, train_min, train_max)\n",
    "processed_val_data = rescale(val_data, -1,1, train_min, train_max)\n",
    "processed_test_data = rescale(test_data, -1,1, train_min, train_max)\n",
    "\n",
    "print \"subtracted mean and normalized data to [-1,1]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Slicing into examples...\n",
      "\n",
      "slicing train\n",
      "examples generated\n",
      "slicing test\n",
      "examples generated\n",
      "slicing val\n",
      "examples generated\n",
      "pickle dumped\n"
     ]
    }
   ],
   "source": [
    "train_test_val = [processed_train_data, processed_test_data, processed_val_data]\n",
    "# =============\n",
    "# make examples by taking overlapping slices of data\n",
    "# =============\n",
    "\n",
    "print '\\n\\nSlicing into examples...\\n'\n",
    "full_dataset = []\n",
    "for what_dataset,dataset in enumerate(train_test_val):\n",
    "    if what_dataset == 0:\n",
    "        wha = 'train'\n",
    "    elif what_dataset == 1:\n",
    "        wha = 'test'\n",
    "    elif what_dataset == 2:\n",
    "        wha = 'val'\n",
    "    else:\n",
    "        print 'well something went wrong'\n",
    "        break;\n",
    "    print 'slicing '+ wha\n",
    "    i = 0\n",
    "    examples = []\n",
    "    current_x = []\n",
    "    current_seq = []\n",
    "    while not i*x_length >= len(dataset)-(x_length + seq_length*x_length):\n",
    "        current_x = np.asarray(dataset[i*x_length : (i+1)*x_length])\n",
    "        current_seq.append(current_x)\n",
    "        if len(current_seq) == seq_length:\n",
    "            examples.append([current_x, current_seq])\n",
    "            current_seq = []\n",
    "        i+=1\n",
    "    full_dataset.append(examples)\n",
    "    print 'examples generated'\n",
    "\n",
    "with open('dataset.pkl', \"wb\") as f:\n",
    "    pickle.dump(full_dataset, f, pickle.HIGHEST_PROTOCOL )\n",
    "    print 'pickle dumped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "\n",
      "song.wav\n",
      "16000 fps, 174229537 frames\n",
      "10889 seconds (181 minutes)\n",
      "\n",
      "\n",
      "Preprocessing...\n",
      "\n",
      "cut up in 80:10:10 train:test:val split\n",
      "test min greater than train min; using train min to normalize\n",
      "test max less than train max; using train max to normalize\n",
      "val min greater than train min; using train min to normalize\n",
      "val max less than train max; using train max to normalize\n",
      "subtracted mean and normalized data to [-1,1]\n"
     ]
    }
   ],
   "source": [
    "# =============\n",
    "# make theano tensors and cut into minibatches\n",
    "# =============\n",
    "full_dataset.side\n",
    "train = theano.tensor()\n",
    "print \"made batches, wished for cookies\"\n",
    "\n",
    "dataset = [processed_train_data, processed_test_data, processed_val_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scaleBetween(unscaledNum, minAllowed, maxAllowed, min, max):\n",
    "    return (maxAllowed-minAllowed)*(unscaledNum-min)/(max - min) + minAllowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.882"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaleBetween(59.0,-1,1,0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.000000 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print '\\n\\nSetting up network...'\n",
    "\n",
    "print '\\nHyperparameters:'\n",
    "\n",
    "n_epochs = 10\n",
    "print \"    Epochs: \"+ str(n_epochs)\n",
    "\n",
    "\n",
    "# T x batches x F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_start = 0.9\n",
    "valid_start = 0.8\n",
    "\n",
    "#load data\n",
    "track = wave.read('song.wav')\n",
    "raw_data = track[1]\n",
    "\n",
    "num_frames = len(raw)\n",
    "data = np.asarray(data, dtype=\"float32\")\n",
    "\n",
    "# if x_length == 0.5, then\n",
    "# x_length*fs = 16000*0.5 = 8000\n",
    "# so an x^i represents 0.5 of a second\n",
    "x_length = float(sys.argv[1])\n",
    "print \"each x will represent %f samples (%f seconds)\" % (x_length*fs, x_length)\n",
    "# a sequence is x_length*fs*seq_length long\n",
    "seq_length = int(sys.argv[2])\n",
    "print \"each sequence will represent %f seconds\" % (x_length*seq_length)\n",
    "\n",
    "how_many_seconds = int(sys.argv[3]) # 60*20 = 20 minutes\n",
    "\n",
    "if how_many_seconds > 0:\n",
    "    sys.stderr.write(\"truncating data to %i seconds\\n\" % how_many_seconds)\n",
    "    data = data[0: how_many_seconds*fs]\n",
    "\n",
    "# e.g. 0 -> 0.8\n",
    "train_data = data[ 0 : len(data)*valid_start ]\n",
    "# e.g. 0.8 -> 0.9 \n",
    "valid_data = data[ len(data)*valid_start : len(data)*test_start ]\n",
    "# e.g. 0.9 ::\n",
    "test_data = data[ len(data)*test_start :: ]\n",
    "\n",
    "mean_ = np.mean(data)\n",
    "min_ = np.min(data)\n",
    "max_ = np.max(data)\n",
    "\n",
    "print \"min and max calculated: %i, %i\" % (min_, max_)\n",
    "\n",
    "train_data = (train_data - mean_) / (max_ - min_)\n",
    "valid_data = (valid_data - mean_) / (max_ - min_)\n",
    "test_data = (test_data - mean_) / (max_ - min_)\n",
    "\n",
    "dd = [train_data, valid_data, test_data]\n",
    "\n",
    "for i in range(0, len(dd)):\n",
    "    b = 0\n",
    "    x_size = int(x_length*fs)\n",
    "    batches = []\n",
    "    seq = []\n",
    "    while True:\n",
    "        if b*x_size >= dd[i].shape[0]:\n",
    "            break\n",
    "        this_x = dd[i][b*x_size : (b+1)*x_size]\n",
    "        seq.append(this_x)\n",
    "        if len(seq) == seq_length:\n",
    "            batches.append(seq)\n",
    "            seq = []\n",
    "        b += 1\n",
    "    dd[i] = np.asarray(batches, dtype=\"float32\")\n",
    "    print \"the shape of this array: %s\" % (str(dd[i].shape))\n",
    "\n",
    "with open(sys.argv[4], \"wb\") as f:\n",
    "    pickle.dump( (dd, min_, max_), f, pickle.HIGHEST_PROTOCOL )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
